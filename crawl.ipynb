{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.exceptions import ProxyError,SSLError ,ChunkedEncodingError\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from numpy.random import default_rng\n",
    "from random import randint\n",
    "from numpy import nan \n",
    "\n",
    "\n",
    "# from emoji import replace_emoji #for remove emoji from description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Extract_link_page:\n",
    "    def __init__(self,end=300 ,start=0 ,primary_link=\"https://kilid.com/buy/tehran?listingTypeId=1&page=\") :\n",
    "        self.start_pages = start\n",
    "        self.end_pages=end\n",
    "        self.primary_link = primary_link\n",
    "        self.links=None\n",
    "    def Extract(self,page_count_auto_save=50,save_end=False,show_time=False ,return_links=False, show_page=False,show_duplicate=False,random_request=False):\n",
    "        \n",
    "        if show_time: t=time.time() ; t1=time.time()\n",
    "        \n",
    "        \n",
    "        link_page=[]\n",
    "        \n",
    "           \n",
    "        for i in range(self.start_pages,self.end_pages):\n",
    "                # time.sleep(1)\n",
    "                # if break_>10:break\n",
    "                try:\n",
    "                    html=requests.get(self.primary_link+str(i))\n",
    "                except:\n",
    "                        sleep=randint(40,120)\n",
    "                        print(f\"erore conection ################ sleep= {sleep}\")\n",
    "                        print(sleep)\n",
    "                        time.sleep(sleep)\n",
    "                        i-=1\n",
    "                else:\n",
    "                    if html.status_code == 200:\n",
    "\n",
    "                        soup=BeautifulSoup(html.content,'html.parser')\n",
    "\n",
    "                        if show_page :print(\"page :\" , i)\n",
    "\n",
    "                        links=soup.find_all('a',class_=\"kilid-listing-card flex-col al-start ng-star-inserted\", href=True)\n",
    "                        if len(links)<10: print(\"The number of links is low\") ; continue\n",
    "\n",
    "                        # else:print(links)\n",
    "                        for counter,link in enumerate(links):\n",
    "                            # sleep(0.2)\n",
    "                            if link[\"href\"]:\n",
    "                                if (link[\"href\"] in link_page) or(link[\"href\"] in self.links):\n",
    "                                    if show_duplicate:print(\"duplicate in page :%d number : %d\"%(i,counter))\n",
    "                                    continue\n",
    "                                else:\n",
    "                                    link_page.append(link[\"href\"])\n",
    "                                # print(\"Found the URL:\", a['href'])\n",
    "                            else:\n",
    "                                print(\"not_found_tag \")\n",
    "                                continue\n",
    "                    if i!=0:\n",
    "                    \n",
    "                        file_name = \"link_total_%s.txt\"%time.strftime(\"%B_%d %H_%M_%S\")\n",
    "\n",
    "                        with open(file_name,\"w\") as f:\n",
    "                            f.write(\" \".join(link_page))\n",
    "                            f.close()          \n",
    "\n",
    "                    file_name_list =self.name_last_file(return_list=True)\n",
    "                    \n",
    "                    if len(file_name_list) > 2:\n",
    "                        \n",
    "                           os.remove(file_name_list[-2])\n",
    "\n",
    "\n",
    "                    if random_request:    \n",
    "                        rand=  default_rng().uniform(0,1) \n",
    "                        time.sleep(rand)\n",
    "                        print('random request time = %d' %rand) \n",
    "                    if show_time:\n",
    "                        print(f'time this request = {round(time.time()-t,3)}')\n",
    "                        t=time.time()\n",
    "                        print(f\"total time = {round(time.time()-t1,3)}\")\n",
    "        if save_end:\n",
    "            with open(\"link_total_%s.txt\"%time.strftime(\"%B_%d %H_%M_%S\"),\"w\") as f:\n",
    "                    f.write(\" \".join(link_page))\n",
    "                    f.close()  \n",
    "            \n",
    "        if return_links:\n",
    "            return link_page\n",
    "            # print(link_home)\n",
    "            \n",
    "    def name_last_file(self,prefix_name=\"link_total\",pos=-1,return_list=False):\n",
    "                files_name=[filename for filename in os.listdir('.') if filename.startswith(prefix_name)]\n",
    "                if files_name and not return_list:\n",
    "                  try:\n",
    "                   return  files_name[pos]\n",
    "                  except IndexError:\n",
    "                     return None\n",
    "                elif return_list : return files_name\n",
    "                return None\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t1 = Extract_link_page(300, \"https://kilid.com/buy/tehran?listingTypeId=1&page=\")\n",
    "# t1.start_pages = 0\n",
    "\n",
    "# last_file=t1.name_last_file(prefix_name=\"link\")\n",
    "# if last_file:\n",
    "#     with open(last_file,\"r\") as f:\n",
    "#                     t1.links=f.read().split()\n",
    "# t1.Extract(page_count_auto_save=1 ,show_duplicate=True,show_page=True,show_time=True,random_request=True,return_links=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import BadZipFile\n",
    "\n",
    "\n",
    "class Extract_informations:\n",
    "   \n",
    "      def __init__(self,links):\n",
    "         self.list_links= links\n",
    "         self.start=None\n",
    "      \n",
    "\n",
    "      def replace_chars(self,text):\n",
    "         for ch in ['/','`','*','_','{','}','[',']','(',')','>','#','+','-','!','$','\\'',\"//\"]:\n",
    "              if ch in text:\n",
    "                  text = text.replace(ch,\" \")\n",
    "         return text\n",
    "\n",
    "      def try_css_selector(self,soup,css):\n",
    "\n",
    "          try:\n",
    "              return self.replace_chars(soup.select_one(css).text.strip())\n",
    "\n",
    "          except AttributeError:\n",
    "              return None\n",
    "      \n",
    "      def extract_location(self,soup,css):\n",
    "         \n",
    "         try:\n",
    "            location=dict()\n",
    "            total_loc=[]\n",
    "            total_loc = self.try_css_selector(soup,css).split(\"ØŒ\")\n",
    "         except: \n",
    "            location[\"region\"]=None\n",
    "            location[\"city\"]=None\n",
    "            location[\"address\"]=None\n",
    "            return location\n",
    "         else:\n",
    "            region=[]\n",
    "            for i in total_loc[1].strip():\n",
    "               if i.isdigit():\n",
    "                  region.append(i)\n",
    "            try: location[\"city\"]=total_loc[0].strip()\n",
    "            except :  location[\"city\"]=None\n",
    "            try:  location[\"region\"]=\"\".join(region)\n",
    "            except: location[\"region\"]=None\n",
    "            try:  location[\"address\"]=total_loc[2].strip()\n",
    "            except:  location[\"address\"]=None\n",
    "\n",
    "            return location\n",
    "         \n",
    "\n",
    "      def extract_features(self,soup,css):\n",
    "         features=dict()\n",
    "         features2=dict()\n",
    "         name=[]\n",
    "         nums=[]\n",
    "         try:\n",
    "            for i in self.try_css_selector(soup,css).split():\n",
    "               if i.isalpha():\n",
    "                  name.append(i)\n",
    "               else:nums.append(i)\n",
    "\n",
    "            features={k:v for k,v in zip(name,nums)}\n",
    "\n",
    "            features2[\"parking\"] = features.get(\"Ù¾Ø§Ø±Ú©ÛŒÙ†Ú¯\",None)    \n",
    "            features2[\"Meterage\"] = features.get(\"Ù…ØªØ±\",None)    \n",
    "            features2[\"bedroom \"] = features.get(\"Ø®ÙˆØ§Ø¨Ù‡\",None)    \n",
    "            features2[\"age_year\"] = features.get(\"Ø³Ø§Ù„Ù‡\",None)    \n",
    "\n",
    "         except:\n",
    "            features2[\"parking\"] = None \n",
    "            features2[\"meterage\"] = None\n",
    "            features2[\"bedroom \"] = None    \n",
    "            features2[\"age_year\"] = None    \n",
    "\n",
    "         return features2\n",
    "\n",
    "\n",
    "      def extract_persian_text(self,soup, css):\n",
    "         try:\n",
    "            string_=str(soup.select_one(css))\n",
    "\n",
    "            facilities=[]\n",
    "            pattern =r'.*>([\\u0600-\\u06FF]+\\s*[\\u0600-\\u06FF]*)<.*'\n",
    "            while True:\n",
    "                try:\n",
    "                    y=re.search(pattern,string_)\n",
    "                    if y.groups():\n",
    "                        facilities.append(y.groups()[0])\n",
    "                        string_=string_.replace(facilities[-1],\"\")          \n",
    "                    else:break\n",
    "\n",
    "                except  : break\n",
    "\n",
    "\n",
    "            return \" , \".join(facilities)\n",
    "\n",
    "         except: return None\n",
    "\n",
    "      def extract_number(self,soup,css):\n",
    "         try:\n",
    "            for i in self.try_css_selector(soup=soup,css=css).split():\n",
    "               if i.isdigit():\n",
    "                  return int(i)\n",
    "         except:\n",
    "            return None\n",
    "\n",
    "\n",
    "      def details(self,href):\n",
    "         try:\n",
    "            html = requests.get(href)\n",
    "            \n",
    "         except (ProxyError,SSLError):\n",
    "            sleep=randint(40,120)\n",
    "            print(f\"erore conection ################ sleep= {sleep}\")\n",
    "            print(sleep)\n",
    "            time.sleep(sleep)\n",
    "            return self.details(href)\n",
    "         except ChunkedEncodingError: \n",
    "            return None , None\n",
    "         else:\n",
    "            html_status = html.status_code == 200\n",
    "            if html_status:\n",
    "               soup = BeautifulSoup(html.content , \"html.parser\")\n",
    "\n",
    "               home=dict() \n",
    "               home[\"title\"]= self.try_css_selector(soup,\".single-data__info\")  \n",
    "               try:\n",
    "                  home[\"total_price\"]=self.replace_chars(self.try_css_selector(soup,\".single-data__container.ng-star-inserted\").split()[2])\n",
    "               except:\n",
    "                  home[\"total_price\"]=None\n",
    "\n",
    "               home[\"price_per_meter\"]=self.extract_number(soup=soup,css=\".ng-star-inserted+ .single-data__container\")\n",
    "\n",
    "               # home=self.merge_dic(home,self.extract_location(soup=soup , css=\".single-data__location span\"))\n",
    "               loc = self.extract_location(soup,\".single-data__location span\")\n",
    "               home=home|loc\n",
    "               # home=self.merge_dic(home,self.extract_features(soup,\".single-data__location span\"))\n",
    "               features= self.extract_features(soup,\".single-data__container--attributes\")\n",
    "               home=home|features\n",
    "               home[\"facilities\"] = self.extract_persian_text(soup,\".ng-trigger-slideDown\")\n",
    "               home[\"adviser\"] = self.try_css_selector(soup=soup , css= \".single-sticky__department__user-name\")\n",
    "               home[\"real_estate\"] =self.try_css_selector(soup=soup,css=\".single-sticky__department__name\")\n",
    "               home[\"ad_code\"] = self.extract_number(soup=soup,css=\".single-sticky__info__item:nth-child(1)\")\n",
    "               # home[\"description\"] = self.extract_persian_text(soup,\".single-description\")\n",
    "               # try: home[\"description\"] = replace_emoji(soup.select_one(\".single-description\").text.strip())\n",
    "               # except:   home[\"description\"] = None\n",
    "\n",
    "\n",
    "               return home , html_status\n",
    "            return None , None\n",
    "\n",
    "                  \n",
    "      def scrap_with_start_end(self,df=pd.DataFrame(),n_pre_scrap=None,myfile_pre=None,primary_link =\"https://kilid.com\" ,start=10000,end=21000,auto_end=False,random_request=True,show_time=True):\n",
    "               \n",
    "               if auto_end:\n",
    "                  end = start+1000\n",
    "               \n",
    "               if show_time: t=time.time() ; t1=time.time()   \n",
    "               counter = 0\n",
    "               for url in range(start,end):\n",
    "                  if n_pre_scrap:\n",
    "                     print(\"link number = %d counter = %d\" %(url+n_pre_scrap,counter))\n",
    "                  else:\n",
    "                     print(\"link number = %d counter = %d\" %(url,counter))\n",
    "                  href = primary_link+self.list_links[url]\n",
    "                  \n",
    "                  detail,html_status=self.details(href)\n",
    "                  if detail and html_status:\n",
    "                     output=pd.DataFrame(detail,index=[0])\n",
    "                     print(detail[\"title\"])\n",
    "                     if output.isna().sum().sum() >10: continue\n",
    "                     myfile = \"homes_not_clean_%s.xlsx\"%time.strftime(\"%B_%d %H_%M_%S\")\n",
    "                     df=pd.concat([df,output],axis=0,ignore_index=True)\n",
    "                     df.to_excel(myfile,index=None)\n",
    "                     \n",
    "                     myfile_pre =self.name_last_excel(pos=-2)\n",
    "                     if myfile_pre :\n",
    "                        os.remove(myfile_pre)\n",
    "                           \n",
    "                  else:\n",
    "                     txt_name=self.name_last_excel(prefix_name=\"link_total\")\n",
    "                     \n",
    "                     with open(txt_name,\"w\") as f:\n",
    "                           self.list_links.pop(url)\n",
    "                           \n",
    "                           f.write(\" \".join(self.list_links))\n",
    "                           f.close()  \n",
    "                           \n",
    "                     \n",
    "                     rand=  default_rng().uniform(1,4) \n",
    "                     print('null request random request time = %d' %rand)    \n",
    "                     # time.sleep(rand)\n",
    "                     continue\n",
    "\n",
    "                  if random_request:    \n",
    "                     rand=  default_rng().uniform(random_request[0],random_request[1]) \n",
    "                     print('random request time = %d' %rand)    \n",
    "                     time.sleep(rand)\n",
    "                     \n",
    "                  if show_time:\n",
    "                     print(f'time this request = {round(time.time()-t,3)}')\n",
    "                     t=time.time()\n",
    "                     print(f\"total time = {round(time.time()-t1,3)}\")\n",
    "                     \n",
    "                     # print(myfile +\" \"+len(files_name) )\n",
    "                        \n",
    "\n",
    "                     # print(len(files_name))\n",
    "                    \n",
    "                  \n",
    "                  \n",
    "                  \n",
    "               \n",
    "            \n",
    "      def data_cleaning(self,df):\n",
    "        \n",
    "         x=[]\n",
    "         \n",
    "         for i in df.facilities[df.facilities.notna()].str.split(\" , \"):\n",
    "           for j in i:\n",
    "         \n",
    "             x.append(j)\n",
    "         x=list(set(x))\n",
    "         len(x)\n",
    "         y =\"\"\"'sauna',\n",
    "           'proportionate shares',\n",
    "           'roof garden',\n",
    "           'balcony',\n",
    "           'sports hall',\n",
    "           'exchange',\n",
    "           'guardian',\n",
    "           'agreed price',\n",
    "           'Elevator',\n",
    "           'Jacuzzi',\n",
    "           'have loan',\n",
    "           'conference hall',\n",
    "           'newly built',\n",
    "           'mall',\n",
    "           'lobby',\n",
    "           'remote door',\n",
    "           'air conditioning',\n",
    "           'pool',\n",
    "           'Central antenna',\n",
    "           'Warehouse'\n",
    "           \"\"\"\n",
    "\n",
    "         dict_unique_features=dict(zip(x,[i.strip().replace(\" \",\"_\") for i in y.replace(\"\\n\",\"\").replace(\"'\",\"\").split(\", \")]))\n",
    "         dict_unique_features\n",
    "\n",
    "         temp=dict()\n",
    "         for i in range(df.shape[0]):\n",
    "           list_features=[]\n",
    "           for j in dict_unique_features.keys():\n",
    "             if df.facilities[i] is nan: \n",
    "                 list_features.append(nan)\n",
    "                 continue\n",
    "              \n",
    "             if j in df.facilities[i].split(\" , \"):\n",
    "               list_features.append(1)\n",
    "             else :\n",
    "               list_features.append(0)\n",
    "           temp[i]=list_features\n",
    "         facilities=pd.DataFrame(temp.values(),columns=dict_unique_features.values(),index=temp.keys())\n",
    "\n",
    "         \n",
    "         del temp\n",
    "         del list_features\n",
    "         \n",
    "         df2=pd.concat([df,facilities],axis=1).drop(\"facilities\",axis=1)\n",
    "         myfile = \"homes_cleaned_%s.xlsx\"%time.strftime(\"%B_%d %H_%M_%S\")\n",
    "         df2.to_excel(myfile,index=None)\n",
    "         \n",
    "         return df2\n",
    "\n",
    "      def continue_previous_req(self,start):\n",
    "         \n",
    "                files_name=self.name_last_excel(\"homes_not_clean_\")\n",
    "                print(files_name)\n",
    "                if files_name:\n",
    "                   df = pd.read_excel(files_name)\n",
    "                   link_number=\"/buy/detail/\" +str(int( df.iloc[-1,:][\"ad_code\"]))\n",
    "                else:\n",
    "                   files_name=None\n",
    "                   df = None\n",
    "                   link_number = None\n",
    "\n",
    "                if link_number :\n",
    "                   try:\n",
    "                      start = self.list_links.index(link_number) + 1\n",
    "                   except:\n",
    "                      pass\n",
    "                   \n",
    "                return start , files_name , df\n",
    "             \n",
    "      def name_last_excel(self,prefix_name=\"homes_not_clean_\",pos=-1,return_list=False):\n",
    "                files_name=[filename for filename in os.listdir('.') if filename.startswith(prefix_name)]\n",
    "                if files_name and not return_list:\n",
    "                  try:\n",
    "                   return  files_name[pos]\n",
    "                  except IndexError:\n",
    "                     return None\n",
    "                elif return_list : return files_name\n",
    "                \n",
    "                return None\n",
    "         \n",
    "      def remove_link(self,pos=-1):\n",
    "         try:\n",
    "            files_name = self.name_last_excel(\"homes_not_clean_\",pos=pos)\n",
    "            \n",
    "            if files_name:\n",
    "               df = pd.read_excel(files_name)\n",
    "               make_link=lambda i : \"/buy/detail/\" + str(int(i))\n",
    "\n",
    "               df.dropna(subset=[\"ad_code\"],inplace=True)\n",
    "               pre_link = df.ad_code.apply(make_link).tolist()\n",
    "               new_link = list(filter (lambda i : i not in pre_link , self.list_links))\n",
    "               self.list_links = new_link\n",
    "               len_ = len(pre_link)\n",
    "            else :  \n",
    "               len_ = 0\n",
    "               \n",
    "            return len_\n",
    "         except BadZipFile:\n",
    "            list_name =name_last_file(return_list=True)\n",
    "            if list_name:\n",
    "               if len(list_name) >1 :\n",
    "                  \n",
    "                  return self.remove_link(pos=pos-1)\n",
    "            print(\"Excel file have a problem please check Excels in this folder\")\n",
    "            \n",
    "               \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # with open(\"link_total_19_15_32.txt\",\"r\") as f:\n",
    "  \n",
    "# #       link_homes=f.read().split()\n",
    "      \n",
    "# # homes=Extract_informations(link_homes)\n",
    "\n",
    "# random_request_time=(0,1)\n",
    "# # change start , end ğŸ‘‡ğŸ‘‡ğŸ‘‡ğŸ‘‡\n",
    "# start  = 10000 \n",
    "# end = 16000    #len(link_homes)\n",
    "\n",
    "# # n_pre_scrap=homes.remove_link()\n",
    "\n",
    "# # start ,myfile_pre, df = homes.continue_previous_req(start)\n",
    "\n",
    "\n",
    "# # homes.scrap_with_start_end(df=df ,n_pre_scrap=n_pre_scrap,myfile_pre=myfile_pre,start= start,end=end  ,random_request=random_request_time,show_time=True)\n",
    "           \n",
    "\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18467 32\n",
      "link number = 18467\n",
      "ÙØ±ÙˆØ´ Ø¢Ù¾Ø§Ø±ØªÙ…Ø§Ù† 130 Ù…ØªØ±, 3 Ø®ÙˆØ§Ø¨Ù‡, Ø´Ù‡Ø±Ú© ØºØ±Ø¨\n",
      "random request time = 5\n",
      "time this request = 14.099\n",
      "total time = 14.099\n",
      "link number = 18468\n",
      "ÙØ±ÙˆØ´ Ø¢Ù¾Ø§Ø±ØªÙ…Ø§Ù† 122 Ù…ØªØ±, 3 Ø®ÙˆØ§Ø¨Ù‡, Ø´Ù‡Ø±Ú© ØºØ±Ø¨\n",
      "random request time = 5\n",
      "time this request = 13.971\n",
      "total time = 28.07\n",
      "link number = 18469\n",
      "Ø¢Ù¾Ø§Ø±ØªÙ…Ø§Ù† 100 Ù…ØªØ±ÛŒ Ù†ÙˆØ³Ø§Ø² ÙØ±ÙˆØ´ÛŒ Ø³Ø¹Ø§Ø¯Øª Ø¢Ø¨Ø§Ø¯\n",
      "random request time = 6\n",
      "time this request = 13.202\n",
      "total time = 41.272\n",
      "link number = 18470\n",
      "ÙØ±ÙˆØ´ Ø¢Ù¾Ø§Ø±ØªÙ…Ø§Ù† 125 Ù…ØªØ±, 3 Ø®ÙˆØ§Ø¨Ù‡, Ø³Ø¹Ø§Ø¯Øª Ø¢Ø¨Ø§Ø¯\n",
      "random request time = 3\n",
      "time this request = 13.263\n",
      "total time = 54.535\n",
      "link number = 18471\n",
      "ÙØ±ÙˆØ´ Ø¢Ù¾Ø§Ø±ØªÙ…Ø§Ù† 135 Ù…ØªØ±, 3 Ø®ÙˆØ§Ø¨Ù‡, Ø´Ù‡Ø±Ú© ØºØ±Ø¨\n",
      "random request time = 3\n",
      "time this request = 11.092\n",
      "total time = 65.627\n",
      "link number = 18472\n",
      "Ø¢Ù¾Ø§Ø±ØªÙ…Ø§Ù† 160 Ù…ØªØ±, 3 Ø®ÙˆØ§Ø¨, Ù†ÙˆØ³Ø§Ø², Ø³Ø¹Ø§Ø¯Øª Ø¢Ø¨Ø§Ø¯\n",
      "random request time = 4\n",
      "time this request = 13.211\n",
      "total time = 78.838\n",
      "link number = 18473\n",
      "ÙØ±ÙˆØ´ Ø¢Ù¾Ø§Ø±ØªÙ…Ø§Ù† 145 Ù…ØªØ± 3 Ø®ÙˆØ§Ø¨Ù‡ Ø´Ù‡Ø±Ú© ØºØ±Ø¨\n",
      "random request time = 6\n",
      "time this request = 15.254\n",
      "total time = 94.092\n",
      "link number = 18474\n",
      "ÙØ±ÙˆØ´ Ø¢Ù¾Ø§Ø±ØªÙ…Ø§Ù† 110 Ù…ØªØ±, 3 Ø®ÙˆØ§Ø¨Ù‡, Ø´Ù‡Ø±Ú© ØºØ±Ø¨\n",
      "random request time = 3\n",
      "time this request = 11.85\n",
      "total time = 105.943\n",
      "link number = 18475\n",
      "70Ù…ØªØ± 2Ø®ÙˆØ§Ø¨ ØºØ±Ù‚ Ù†ÙˆØ± ÙØ±Ø¯ÙˆØ³ ØºØ±Ø¨\n",
      "random request time = 5\n",
      "time this request = 15.632\n",
      "total time = 121.575\n",
      "link number = 18476\n",
      "Ø¢Ù¾Ø§Ø±ØªÙ…Ø§Ù† 71 Ù…ØªØ±ÛŒ ÙÙˆÙ„ Ø¨Ø§Ø²Ø³Ø§Ø²ÛŒ Ù¾ÙˆÙ†Ú©\n",
      "random request time = 4\n",
      "time this request = 13.432\n",
      "total time = 135.006\n",
      "link number = 18477\n",
      "70Ù…ØªØ±ÛŒ Ø¬Ù†Øª Ø¬Ù†ÙˆØ¨ Ø¨Ø§Ø²Ø³Ø§Ø²ÛŒ Ø´Ø¯Ù‡\n",
      "random request time = 5\n",
      "time this request = 15.291\n",
      "total time = 150.297\n",
      "link number = 18478\n",
      "69Ù…ØªØ± 2Ø®ÙˆØ§Ø¨ Ø¨Ø§Ø²Ø³Ø§Ø²ÛŒ Ø´Ø¯Ù‡ ÙØ±Ø¯ÙˆØ³ ØºØ±Ø¨\n",
      "random request time = 6\n",
      "time this request = 14.717\n",
      "total time = 165.015\n",
      "link number = 18479\n",
      "65Ù…ØªØ± 2Ø®ÙˆØ§Ø¨ ÙÙˆÙ„ Ø§Ù…Ú©Ø§Ù†Ø§Øª Ø³Ø§Ø²Ù…Ø§Ù† Ù…Ø±Ú©Ø²ÛŒ\n",
      "random request time = 5\n",
      "time this request = 15.817\n",
      "total time = 180.847\n",
      "link number = 18480\n",
      "ÙØ±ÙˆØ´ Ø¢Ù¾Ø§Ø±ØªÙ…Ø§Ù† Ø¨Ø±Ø¬ 2 Ø®ÙˆØ§Ø¨Ù‡ Ø¯Ø± Ø¬Ù†Øª Ø§Ø¨Ø§Ø¯ Ù…Ø±Ú©Ø²ÛŒ\n",
      "random request time = 6\n",
      "time this request = 18.489\n",
      "total time = 199.336\n",
      "link number = 18481\n",
      "Ø¢Ù¾Ø§Ø±ØªÙ…Ø§Ù† 85Ù…ØªØ± 2Ø®ÙˆØ§Ø¨ ÙÙˆÙ„ Ø§Ù…Ú©Ø§Ù†Ø§Øª Ø²ÛŒØ± Ù‚ÛŒÙ…Øª Ù…Ù†Ø·Ù‚Ù‡ Ù„ÙˆÚ©Ø³\n",
      "random request time = 4\n",
      "time this request = 12.866\n",
      "total time = 212.202\n",
      "link number = 18482\n",
      "Ø¢Ù¾Ø§Ø±ØªÙ…Ø§Ù† 130Ù…ØªØ± 3Ø®ÙˆØ§Ø¨ ÙÙˆÙ„ Ø§Ù…Ú©Ø§Ù†Ø§Øª Ø³Ø§Ù„Ù† Ù¾Ø±Ø¯Ù‡ Ø®ÙˆØ± Ø²ÛŒØ± Ù‚ÛŒÙ…Øª\n",
      "random request time = 5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(Extract_link_page.name_last_file(Extract_link_page),\"r\") as f:\n",
    "\n",
    "  \n",
    "      link_homes=f.read().split()\n",
    "      \n",
    "homes=Extract_informations(link_homes)\n",
    "\n",
    "random_request_time=(3,7)\n",
    "# change start , end ğŸ‘‡ğŸ‘‡ğŸ‘‡ğŸ‘‡\n",
    "   #len(link_homes)\n",
    "\n",
    "n_pre_scrap=homes.remove_link()\n",
    "\n",
    "start  = 0 \n",
    "end =len(homes.list_links)\n",
    "print(n_pre_scrap,len(homes.list_links))\n",
    "\n",
    "df =pd.read_excel( homes.name_last_excel())\n",
    "myfile_pre = homes.name_last_excel()\n",
    "\n",
    "homes.scrap_with_start_end(df=df ,n_pre_scrap=n_pre_scrap,myfile_pre=myfile_pre,start= start,end=end ,random_request=random_request_time,show_time=True)\n",
    "           \n",
    "\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name1=homes.name_last_excel(pos=-2)\n",
    "# name2=homes.name_last_excel()\n",
    "\n",
    "# df1=pd.read_excel(name1)\n",
    "# df2=pd.read_excel(name2)\n",
    "# print(df1.shape,df2.shape)\n",
    "# df3 = pd.concat([df1,df2],axis=0,ignore_index=True)\n",
    "# print(df3.shape)\n",
    "# df3.drop_duplicates(inplace=True)\n",
    "# print(df3.shape)\n",
    "# myfile = \"homes_not_clean_%s.xlsx\"%time.strftime(\"%B_%d %H_%M_%S\")\n",
    "# df3.to_excel(myfile,index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "region\n",
       "1.0     492\n",
       "2.0     443\n",
       "3.0     402\n",
       "4.0      67\n",
       "5.0     997\n",
       "6.0      62\n",
       "7.0     244\n",
       "8.0       8\n",
       "9.0      25\n",
       "10.0    137\n",
       "11.0     18\n",
       "12.0      4\n",
       "13.0     47\n",
       "17.0      3\n",
       "19.0      1\n",
       "20.0     22\n",
       "21.0      4\n",
       "22.0     11\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df=pd.read_excel(homes.name_last_excel())\n",
    "# df.region.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after finish work run this\n",
    "'''\n",
    "\n",
    "df = pd.read_excel(df.read_homes.name_last_excel(\"homes_not_clean_\"))\n",
    "df_clean = homes.data_cleaning(df)\n",
    "df_clean.to_excel(\"homes_cleaned.xlsx\",index=None)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
